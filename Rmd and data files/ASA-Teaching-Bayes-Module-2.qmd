---
title: "Module 2: Bayesian Inference and Prediction"
author: "[Jingchen (Monika) Hu (Vassar)](https://pages.vassar.edu/jihu/) and [Kevin Ross (Cal Poly)](https://statistics.calpoly.edu/Kevin-Ross)"
format:
  html:
    toc: true
    number-sections: true
    embed-resources: true
---


```{r}
#| warning: false
#| message: false
#| echo: false

library(knitr)

knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = TRUE)

```




```{r}
#| label: package-setup
#| echo: false

library(shinyjs)
library(shinystan)
library(shinythemes)
library(tidyverse)
library(gridExtra)
library(brms)
library(tidybayes)
library(posterior)
library(viridis)

set.seed(21234)

# color for: prior, likelihood, posterior
bayes_col = c("#56B4E9", "#E69F00", "#009E73", "#CC79A7", "#D55E00")
names(bayes_col) = c("prior", "likelihood", "posterior", "prior_predict", "post_predict")

# linetype for: prior, likelihood, posterior
bayes_lty = c("dashed", "dotted", "solid")
names(bayes_lty) = c("prior", "likelihood", "posterior")

```

# Introduction {#sec-intro}

**The activities in this module are suitable for an introductory statistics class.**
*However, we have compressed the material for the workshop.*
*In a course these ideas would be expanded and stretched out over several classes and examples.*

In Module 1 we saw how probability distributions can be used to quantify uncertainty about a single parameter, both before (prior) and after (posterior) observing sample data.
However, almost all interesting problems involve multiple parameters.
When there are multiple parameters, formulating a prior distribution of any single parameter can be difficult.
It is often more helpful to consider the **prior *predictive* distribution** of the response variable when tuning the prior distribution of parameters. 
After observing data, **posterior *predictive* distributions** can also be used for model checking, and for making predictions.


The posterior distribution is the conditional distribution of parameters given the observed data.
In Module 1, we saw two methods for approximating the posterior distribution: grid approximation and simulation.
Grid approximation (Bayes table) is intuitive, but it is usually computationally infeasible, especially in problems with many parameters.
In almost all practical problems the posterior distribution is approximated via simulation.
However, naive simulation like what we did in Module 1---discard all the simulated samples that don't match the observed data---is horribly inefficient.
MCMC algorithms allow us to simulate efficiently from posterior distributions.


This Module will introduce, in the context of inference for a single numerical variable:

- How to simulate "predictive distributions" of the response variable
- How prior predictive distributions can be used to tune prior distributions of multiple parameters
- How software can be used to find posterior distributions of parameters
- How posterior distributions can be used for making inference about parameters
- How posterior predictive distributions can be used for model checking
- How predictive distributions can be used to make predictions (e.g., prediction intervals)







# Setup {#sec-setup}

::: {.panel-tabset}

## Problem context

How late do people arrive at parties?
[FiveThirtyEight](https://fivethirtyeight.com/features/how-to-estimate-when-people-will-arrive-at-a-party/) conducted a survey to address this question.
We'll assume this is a reasonably representative sample and arrival times are measured reliably.
Arrival times are measured in minutes after the scheduled start time, rounded to the nearest minute, with negative arrival times representing arrivals before the scheduled start time.

::: {.callout-note}
Other variables like length of the party, type of party, number of guests, etc. could reasonably be related to arrival times.
In the interest of time, we'll gloss over such issues, but you might want to discuss with students.
For example, should we measure arrival time in absolute terms (minutes after start), or relative terms (fraction of party duration) for parties of varying length?
What "population" might the data be reasonably representative of?
:::


## Code

Here are some R packages we’ll use.

```{r}
#| ref.label: package-setup
#| eval: false

```

:::

# Formulating assumptions and prior predictive tuning (~15 minutes) {#sec-prior-predictive}


Before exploring the sample data, let's consider our assumptions.

::: {.panel-tabset}
## Questions


1.  What would you expect the population distribution of arrival times to look like?
For example, what percent of arrivals would you expect to be early (negative)?
Later than two hours?
What would you expect of other features like shape, center, and variability?


1.  We will start by assuming that, given the values of relevant parameters, arrival times follow a Normal distribution.
Describe what this assumption says about arrival times.
What are the relevant parameters, and what do they represent?
*We will revisit this conditional Normal assumption later, but go with it for now.*


1.  What do you think is a reasonable prior distribution for the population mean $\mu$?
A few things to consider:
    
    - What is your "best guess" for $\mu$?
    - What range of values do you think it is 2 times more plausible for $\mu$ to lie inside rather than outside?
    (This corresponds to a 67% prior credible interval for $\mu$.
    Replacing 2 with 2.1 corresponds to a 68% prior credible interval for $\mu$.)
    - What range of values do you think it is 19 times more plausible for $\mu$ to lie inside rather than outside?
    (This corresponds to a 95% prior credible interval for $\mu$.)
    - What is a reasonable prior standard deviation for $\mu$?
    (Thinking in terms of intervals like the above might help.)
    - Remember, there is no perfect prior; just formulate what you think is a reasonable starting point.  



1.  Repeat the previous part to formulate a reasonable prior distribution for the population standard deviation $\sigma$.
(*It's trickier to formulate priors for standard deviations.*
*Rather than stressing out about prior distributions of individual parameters, we'll focus on prior predictive tuning; see the next parts.*)


1.  Describe how you could simulate many *arrival times* according to the assumptions of your model.
This simulation approximates the **prior *predictive* distribution**.
Careful: our goal here is to simulate values of the *measured variable* (arrival time), and not just values of the parameters.


1.  [This applet](https://kevin-davisross.shinyapps.io/Normal-Prior-Predictive/) conducts the simulation from the previous part.
Move the sliders to enter your prior mean and SD for $\mu$ and $\sigma$.
Does the distribution of arrival times seem reasonable based on your expectations from part 1?
For example, what percent of arrivals are early (negative), or later than two hours---and do these values seem reasonable?
If not, revise your assumptions and try again (i.e., play with the sliders in the applet) until you find a distribution that is reasonable.
Do not worry about getting it perfect; you just want to settle on assumptions that provide a reasonable starting point.

::: {.callout-note}
The assumptions include both the conditional Normal model for arrival times and the prior distribution of parameters.
We're taking the conditional Normal model of arrival times as given for now.
We'll revisit that assumption later.
:::


## Short solution

In a $N(\mu, \sigma)$ distribution, $\mu$ is the population mean arrival time and $\sigma$ is the population standard deviation.
There are many reasonable choices for the prior distribution of parameters.
We'll assume a $N(40, 15)$ prior for the population mean $\mu$, and a $N(30, 10)$ prior for the population standard deviation $\sigma$.

We could simulate an arrival time by first (1) simulating $\mu$ and $\sigma$ from their prior distributions and then (2) simulating $y$ from a $N(\mu, \sigma)$ distribution.
Here are the results of a few repetitions.


```{r}
#| label: prior-predictive-sim
#| echo: false

n_rep = 10000

# prior mean and SD for mu
mu_prior_mean = 40
mu_prior_sd = 15

# prior mean and SD for sigma
sigma_prior_mean = 30
sigma_prior_sd = 10

sim_prior = data.frame(
  # simulate mu and sigma from prior
  mu = rnorm(n_rep, mu_prior_mean, mu_prior_sd),
  sigma = rgamma(n_rep,
                 shape = sigma_prior_mean ^ 2 / sigma_prior_sd ^ 2,
                 rate = sigma_prior_mean / sigma_prior_sd ^ 2)) |>
  
  # given mu and sigma, simulate y from Normal(mu, sigma)
  mutate(y_predict = rnorm(n_rep, mu, sigma))
```


```{r}
#| label: prior-predictive-sim-results
#| echo: false

# display the results of a few repetitions
sim_prior |>
  head() |>
  kable()

```

We can approximate the prior predictive distribution by simulating many values of $y$ in this manner.
Here are some plots of the prior predictive distribution according to our assumed model.

```{r}
#| label: fig-prior-predictive
#| fig-cap: "Prior predictive distribution"
#| echo: false

p1 = sim_prior |>
  ggplot(aes(x = y_predict)) +
  geom_density(col = bayes_col[4]) +
  labs(x = "Arrival time (minutes)",
       title = "Simulated prior predictive distribution of arrival times: Density") +
  scale_x_continuous(breaks = seq(-60, 180, by = 30),
                     limits = c(-60, 180)) +
  theme_bw()

p2 = sim_prior |>
  ggplot(aes(x = y_predict)) +
  stat_ecdf(col = bayes_col[4]) +
  labs(x = "Arrival time (minutes)",
       y = "Cumulative probability",
       title = "Simulated prior predictive distribution of arrival times: CDF") +
  scale_x_continuous(breaks = seq(-60, 180, by = 30),
                     limits = c(-60, 180)) +
  theme_bw()

grid.arrange(p1, p2)


```




We reemphasize that there is no perfect prior distribution, or prior predictive distribution.
Rather, we simply need a prior distribution that provides a reasonable starting point.
What's really important is the posterior distribution, but to get a posterior distribution there needs to be a prior distribution.
Think of the prior distribution as the initialization step in a numerical algorithm; we just need a reasonable starting point.



## Discussion


1.  There are many reasonable responses.


1.  In a $N(\mu, \sigma)$ distribution, $\mu$ is the population mean arrival time and $\sigma$ is the population standard deviation.
The population SD measures the person-to-person variability in arrival times over many party-goers.
We're assuming a Normal shape; e.g., 68% of arrival times are within 1 standard deviation of the mean, 95% within 2 SDs, etc.


1.  There are many reasonable responses.
If we assume a $N(40, 15)$ prior for $\mu$, we're saying that our best guess of the population mean arrival time $\mu$ is 40 minutes, that it's about two times more plausible that the population mean arrival time lies inside [25, 55] than outside, that's it's about 19 times more plausible that the population mean arrival time lies inside [10, 70] than outside, etc.


1.  Again, there are many reasonable responses.
If we assume a $N(30, 10)$ prior for $\sigma$, we're saying that our best guess of the population standard deviation of arrival times is 30 minutes, that it's about two times more plausible that the population SD lies inside [20, 40] than outside, that's it's about 19 times more plausible that the population SD lies inside [10, 50] than outside, etc.


1.  First, simulate a value of $\mu$ from its prior distribution, say $N(40, 15)$, and a value of $\sigma$ from its prior distribution, say $N(30, 10)$.
(*We're assuming $\mu$ and $\sigma$ are independent, otherwise we would simulate a $(\mu, \sigma)$ pair from the joint prior distribution.*)
Given $\mu$ and $\sigma$, simulate an arrival time $y$ from a $N(\mu, \sigma)$ distribution.
That's one repetition.
Repeat many times and summarize the simulated $y$ values to approximate the prior predictive distribution.


1.  See the applet; an example is also shown in the Code tab.
We reemphasize that there is no perfect prior distribution, or prior predictive distribution.
Rather, we simply need a prior distribution that provides a reasonable starting point.
What's really important is the posterior distribution, but to get a posterior distribution there needs to be a prior distribution.
Think of the prior distribution as the initialization step in a numerical algorithm; we just need a reasonable starting point.
*We will investigate the* posterior *predictive distribution in more detail later.*

    In models with multiple parameters, there can be dependencies between parameters, so interpreting the marginal prior distribution of any single parameter can be difficult.
    It is often more helpful to consider predictive distributions, which account for the joint distribution of all parameters (and the model for the data).
    Interpreting predictive distributions is often more intuitive since predictive distributions live on the scale of the measured response variable.


::: {.callout-warning}
Careful! We have already introduced three different Normal distributions, each playing a different role, along with a few standard deviations.
Students can easily get confused by all the distributions.
Be careful to distinguish between prior/posterior distributions of parameters---which quantify uncertainty/plausibility in parameters---and prior/posterior predictive distributions---which represent variability of values of the response variable.
Ask lots of questions like "this distribution/SD measures uncertainty/variability of what?"
:::

::: {.callout-warning}
Careful! We summarize simulated values of parameters to approximate prior/posterior distributions, but we summarize simulated values of the response variable $y$ to approximate prior/posterior *predictive* distributions.
:::

## Code





Here is the main code for the prior predictive simulation, along with a few simulated repetitions.
*Technically, to avoid negative values of $\sigma$, we're assuming a Gamma distribution prior with mean 30 and SD 10.*


```{r}
#| ref.label: !expr c("prior-predictive-sim")
#| eval: false

```

```{r}
#| ref.label: !expr c("prior-predictive-sim-results")

```

Plots of the prior predictive distribution.

```{r}
#| ref.label: !expr c("fig-prior-predictive")

```


Here is some code for creating a rough [applet](https://kevin-davisross.shinyapps.io/Normal-Prior-Predictive/) that you can embed within an RMarkdown file by adding `runtime: shiny` to the YAML metadata; see [Shiny Documents](https://bookdown.org/yihui/rmarkdown/shiny-documents.html).
(This code is not evaluated; you would need to run it on your own.)


```{r}
#| eval: false

sliderInput("mu_prior_mean", "Prior mean of mu:", value = 10, min = -60, max = 120)

sliderInput("mu_prior_sd", "Prior SD of mu:", value = 10, min = 5, max = 120)

sliderInput("sigma_prior_mean", "Prior mean of sigma:", value = 10, min = 5, max = 120)

sliderInput("sigma_prior_sd", "Prior SD of sigma:", value = 10, min = 5, max = 120)

renderPlot({
  
  n_rep = 10000
  
  sim_prior = data.frame(
    mu = rnorm(n_rep, input$mu_prior_mean, input$mu_prior_sd),
    sigma = rgamma(n_rep,
                   shape = input$sigma_prior_mean ^ 2 / input$sigma_prior_sd ^ 2,
                   rate = input$sigma_prior_mean / input$sigma_prior_sd ^ 2)) |>
    mutate(y_predict = rnorm(n_rep, mu, sigma))
  
  p1 = sim_prior |>
    ggplot(aes(x = y_predict)) +
    geom_density(col = bayes_col[4]) +
    labs(x = "Arrival time (minutes)",
         title = "Simulated prior predictive distribution of arrival times: Density") +
    scale_x_continuous(breaks = seq(-60, 180, by = 30),
                         limits = c(-60, 180)) +
    theme_bw()

  p2 = sim_prior |>
    ggplot(aes(x = y_predict)) +
    stat_ecdf(col = bayes_col[4]) +
    labs(x = "Arrival time (minutes)",
         y = "Cumulative probability",
         title = "Simulated prior predictive distribution of arrival times: CDF") +
    scale_x_continuous(breaks = seq(-60, 180, by = 30),
                         limits = c(-60, 180)) +
    theme_bw()
  
  grid.arrange(p1, p2)
    
})

```

:::

# Approximating the posterior distribution via simulation (~5 minutes) {#sec-posterior-simulation}


Your prior distribution is whatever it is and reflects your background knowledge of the situation.
But so that we're all on the same page, let's assume that

- We have a representative random sample of arrival times $y$ (minutes)
- Given $\mu$ and $\sigma$, arrival times follow a Normal distribution with mean $\mu$ and standard deviation $\sigma$
- The prior distribution for $\mu$ is Normal with mean 40 minutes and standard deviation 15 minutes
- The prior distribution for $\sigma$ is a Gamma distribution with mean 30 minutes and standard deviation 10 minutes.

::: {.callout-note}
We're assuming a Gamma distribution since $\sigma > 0$, but we could also use a Normal distribution truncated at 0.
Since students are more familiar with Normal distributions, we often use Normal distributions as priors, especially early in the course.
:::

In symbols, our model assumes: (*We would not require this notation of introductory statistics students*)

\begin{align*}
y_i | \mu, \sigma & \stackrel{\text{i.i.d.}}{\sim} N(\mu, \sigma)\\
\mu & \sim N(40, 15)\\
\sigma & \sim \text{Gamma with mean 30 and SD 10}\\
\mu, \sigma & \quad \text{ are independent}
\end{align*}

Now we'll observe some sample data.
Our goal is to find the posterior distribution of the parameters $\mu$ and $\sigma$ given the data.

For illustration, first suppose that only six arrival times are measured: 4, 8, 15, 16, 23 and 42 minutes (rounded to the nearest minute).

::: {.panel-tabset}

## Questions



1.  In principle, how could you conduct a simulation and use the results to approximate the (joint) posterior distribution of $\mu$ and $\sigma$ given the data?
Hint: remember, the posterior distribution is the conditional distribution of parameters given the data.
Think about what we did in Module 1.


1.  A few repetitions of simulated $(\mu, \sigma)$ pairs, along with simulated samples of 6 arrival times, are displayed below.
What is the practical difficulty with using such a simulation to approximate the posterior distribution?


```{r, indent = "    "}
#| label: naive-sim
#| echo: false

n_rep = 10

data.frame(rep = 1:n_rep,
           
           # simulate mu and sigma from the prior distribution
           mu = rnorm(n_rep, 40, 15),
           sigma = rnorm(n_rep, 30, 10)) |>
  
  # given mu and sigma, simulate a sample of size 6 from N(mu, sigma)
  mutate(samples = map(rep, ~sort(round(rnorm(6, mu, sigma), 0)))) |>
  
  # display the results
  kable(align = 'r', digits = 1)
  
```



1.  In practice we will use sophisticated algorithms to approximate the posterior distribution via simulation.
For now, we'll ignore the details of how the algorithms work; just know that they simulate values from the posterior distribution---that is, the conditional distribution of parameters given the sample data---much more efficiently than our naive simulation above.
Instead, we'll focus on specifying the input and interpreting the output.
Describe what we need to input into the algorithm.
That is, what are the main ingredients we need to find the posterior distribution?
(Hint: there are three; it might help to think about a Bayes table.)



## Short solution


In principle, the posterior distribution of $\mu, \sigma$ given the observed sample (4, 8, 15, 16, 23, 42) can be found via the following.

- Simulate a $(\mu, \sigma)$ pair from the prior distribution
- Given $\mu, \sigma$, simulate a sample of size 6 from a $N(\mu, \sigma)$ distribution (rounded to the nearest minute.)
- If the simulated sample is (4, 8, 15, 16, 23, 42) keep the repetition; otherwise discard it
- Repeat the process until enough non-discarded repetitions are obtained---all corresponding to samples with (4, 8, 15, 16, 23, 42)
- Summarize the simulated $\mu, \sigma$ values to approximate the posterior distribution.

However, the likelihood of producing a sample that matches the observed data is essentially 0, simply because there are so many possible samples.

Regardless of which simulation algorithm we use, there are 3 main inputs

- the data
- a parametric model for the data (which, in conjunction with the data, determines the likelihood), e.g. $y | \mu, \sigma \sim N(\mu, \sigma)$.
- a prior distribution for parameters


## Discussion


In principle, the posterior distribution of $\mu, \sigma$ given the observed sample (4, 8, 15, 16, 23, 42) can be found via the following.

- Simulate a $(\mu, \sigma)$ pair from the prior distribution
- Given $\mu, \sigma$, simulate a sample of size 6 from a $N(\mu, \sigma)$ distribution.
(The values in the simulated sample would need to be rounded to some desired degree of precision, to the nearest minute here.)
- If the simulated sample is (4, 8, 15, 16, 23, 42) keep the repetition; otherwise discard it
- Repeat the process until enough non-discarded repetitions are obtained --- all corresponding to samples with (4, 8, 15, 16, 23, 42)
- Summarize the simulated $\mu, \sigma$ values to approximate the posterior distribution.

However, the likelihood of producing a sample that matches the observed data is essentially 0, simply because there are so many possible samples, even if we condition on the values of sufficient statistics rather than the whole sample.
(Without rounding, the probability would be 0 for a continuous variable.)
While in principle this method works, in practice it would be horribly inefficient since virtually all repetitions would be discarded.
Even if the relatively simple problem in Module 1, in 100,000 repetitions there were only around 5000 that were not discarded.

Therefore, we need more efficient simulation algorithms for approximating posterior distributions.
Markov chain Monte Carlo (MCMC) methods provide powerful and widely applicable algorithms for simulating from probability distributions, including complex and high-dimensional distributions.
These algorithms include Metropolis-Hastings, Gibbs sampling, Hamiltonian Monte Carlo, No U-Turn Sampling (NUTS), among others.
There are many related software packages (e.g., JAGS, Stan), along with many R packages for running MCMC algorithms and performing Bayesian analyses within R (e.g., `rjags`, `runjags`, `brms`, `rstanarm`).
There are also many R packages that can be used to summarize output from a variety of sources (e.g., `bayesplot`, `tidybayes`.)


Regardless of which algorithm we use, there are 3 main inputs

- the data
- a parametric model for the data (which, in conjunction with the data, determines the likelihood), e.g. $y | \mu, \sigma \sim N(\mu, \sigma)$.
- a prior distribution for parameters


## Code


Here is some code for simulating a few samples of size 6.
Because naive simulation of the posterior distribution of parameters is so inefficient, we didn't bother conditioning on the observed data.


```{r}
#| ref.label: !expr c("naive-sim")
#| eval: false

```

:::




# The posterior distribution, and inference about $\mu$ (~10 minutes) {#sec-posterior}


Recall our assumptions from @sec-posterior-simulation.
We will now load the FiveThirtyEight data and use it to fit the model and find the posterior distribution of the parameters $\mu$ and $\sigma$.

The data is contained in the party-time.csv file; the "minutes" variable contains 803 arrival times (rounded to the nearest minute).
(*We couldn't find the raw FiveThirtyEight data, so this data has been simulated to match the histogram in the article.*)



```{r}

data = read.csv("party-time.csv")

```


We should always first explore the sample data, but to illustrate a point we will not do that yet.
Instead, we'll skip straight to fitting the model.

Here is some code for fitting the model to the data using the [`brms` package](https://paul-buerkner.github.io/brms/) ("Bayesian Regression Models using Stan").
The three inputs to `brm` are

- Input 1: the data  
`data = data`

- Input 2: model for the data (likelihood)  
`family = gaussian`  
`minutes ~ 1 # linear model with intercept (mean) only`

- Input 3: prior distribution for parameters  
`prior = c(prior(normal(40, 15), class = Intercept), # prior for mu`  
`prior(gamma(30, 30 ^ 2 / 10 ^ 2), class = sigma)), # gamma(mean, shape) prior for sigma`

- Other arguments specify the details of the numerical algorithm; our specifications below will result in 10000 $(\mu, \sigma)$ pairs simulated from the posterior distribution; a few simulated values are displayed below.
*We'll discuss inputs like `chains = 1` in later modules.*


```{r}
#| label: brm-fit
#| cache: true
#| message: false
#| warning: false

fit <- brm(data = data,
           family = gaussian,
           minutes ~ 1,
           prior = c(prior(normal(40, 15), class = Intercept),
                     prior(gamma(30, 30 ^ 2 / 10 ^ 2), class = sigma)), # gamma(mean, shape)
           iter = 11000,
           warmup = 1000,
           chains = 1,
           refresh = 0)

```


::: {.callout-note}
What we expect of students in terms of coding depends on the class.
In an introductory statistics class, we might not even show students code like this, let alone expect them to write or run it.
Nevertheless, we would expect students to understand the process: what are the inputs and do we interpret the outputs?
Even if students never see any code, we would expect them to describe the Bayesian updating and prediction processes in words, and to read and interpret output and plots like the ones in this module.
:::

Here are a few $(\mu, \sigma)$ pairs simulated from the posterior distribution when the model is fit using `brms`.

```{r}
#| label: posterior-df
#| echo: false

posterior = fit |>
  spread_draws(b_Intercept, sigma) |>
  rename(mu = b_Intercept)

posterior |>
  head() |>
  kable()

```


We will focus on summarizing and interpreting the posterior distribution of $\mu$, using the following plots created based on the `brms` output.


```{r}
#| label: fig-posterior-mu
#| ref.label: !expr c("mu-posterior-plots")
#| echo: false
#| layout-ncol: 2
#| fig-cap:
#|   - "Posterior distribution of mu (pdf)"
#|   - "Posterior distribution of mu (cdf)"

```

::: {.callout-note}
Note: we will see below that a conditional Normal model is not really appropriate for this data, and we will refit the model soon.
In class, we would start with a situation and data where a conditional Normal model is appropriate and inference about $\mu$ is the main concern, and we would cover posterior inference for $\mu$ before covering posterior prediction or posterior predictive checking.
This is why we're asking questions about the posterior distribution of $\mu$ now, even though we're going to refit the model later.
(We would also introduce inference for a population mean by first assuming that $\sigma$ is known.)*
:::

::: {.panel-tabset}
## Questions


1.  Describe the posterior distribution of $\mu$.
What does this tell you in context?


1.  Find and interpret in context a 50% posterior credible interval for $\mu$.


1.  Find and interpret in context an 80% posterior credible interval for $\mu$.


1.  Find and interpret in context a 98% posterior credible interval for $\mu$.


1.  Find and interpret the posterior probability that $\mu$ is greater than 60 minutes.


## Short solution


There is a 98% posterior probability that the population mean arrival time is between `r round(quantile(posterior$mu, 0.01), 0)`  and `r round(quantile(posterior$mu, 0.99), 0)` minutes.
It is 49 times more plausible that population mean arrival time lies inside the interval [`r round(quantile(posterior$mu, 0.01), 0)`, `r round(quantile(posterior$mu, 0.99), 0)`] than outside.

There is a posterior probability of about 99.9% that population mean waiting time is greater than 60 minutes.
(*Of course, this by itself does not necessarily mean $\mu$ is much greater than 60.*)



## Discussion


*Again, we're going to refit the model, so don't put much weight on the results in this section.*
*But they do provide an illustration of Bayesian inference about the population mean.*



1.  The posterior distribution of $\mu$ is approximately Normal with mean `r round(mean(posterior$mu), 0)` minutes and standard deviation `r round(sd(posterior$mu), 1)` minutes.
Our best guess for the population mean arrival time is `r round(mean(posterior$mu), 1)` minutes, and `r round(sd(posterior$mu), 1)` minutes summarizes in a single number our degree of uncertainty about the population mean arrival time.


1.  There is a 50% posterior probability that the population mean arrival time is between `r round(quantile(posterior$mu, 0.25), 0)`  and `r round(quantile(posterior$mu, 0.75), 0)` minutes.
It is equally plausible that population mean arrival time lies inside the interval [`r round(quantile(posterior$mu, 0.25), 0)`, `r round(quantile(posterior$mu, 0.75), 0)`] than outside.


1.  There is an 80% posterior probability that the population mean arrival time is between `r round(quantile(posterior$mu, 0.1), 0)`  and `r round(quantile(posterior$mu, 0.9), 0)` minutes.
It is 4 times more plausible that population mean arrival time lies inside the interval [`r round(quantile(posterior$mu, 0.1), 0)`, `r round(quantile(posterior$mu, 0.9), 0)`] than outside.


1.  There is a 98% posterior probability that the population mean arrival time is between `r round(quantile(posterior$mu, 0.01), 0)`  and `r round(quantile(posterior$mu, 0.99), 0)` minutes.
It is 49 times more plausible that population mean arrival time lies inside the interval [`r round(quantile(posterior$mu, 0.01), 0)`, `r round(quantile(posterior$mu, 0.99), 0)`] than outside.

1.  There is a posterior probability of about 99.9% that population mean waiting time is greater than 60 minutes.
(*Of course, this by itself does not necessarily mean $\mu$ is much greater than 60.*)

::: {.callout-note}
The full posterior distribution provides the most complete description of the uncertainty in the parameters.
Credible intervals and posterior probabilities are summaries of the posterior distribution.
There is no reason why we need to only report a single credible interval, and no reason why we need to use 95% credibility.
When summarizing a posterior distribution with credible intervals, we prefer to report a few intervals (like 50%, 80%, and 98%) to provide a better picture of the shape of the posterior distribution.
:::



## Code

::: {.callout-warning}
[Installing `RStan`](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started) --- which `brms` depends on --- is a little more involved than the usual `install.packages`.
In particular, you can run into versioning issues with [`RTools`](https://github.com/stan-dev/rstan/wiki/Configuring-C---Toolchain-for-Windows).
:::

The brms code is repeated here.

```{r}
#| ref.label: !expr c("brm-fit")
#| eval: false

```


Here are some plots and summaries of the simulated posterior distribution, along with some diagnostics.
*We'll discuss output such as this in later modules.*


```{r}

plot(fit)

```

```{r}

pairs(fit,
      off_diag_args = list(alpha = 0.1))
```


```{r}

summary(fit)

```


There are many packages and functions for working with output from Bayesian analyses.
Using `spread_draws` from the `tidybayes` package, we can put the 10000 simulated values into a data frame.
Then we can summarize the posterior distribution just like we summarize any data set.


```{r}
#| ref.label: !expr c("posterior-df")

```


Here is a ggplot of the posterior distribution, density and cdf, of $\mu$.


```{r}
#| label: mu-posterior-plots


posterior |>
  ggplot(aes(x = mu)) +
  geom_density(col = bayes_col["posterior"]) +
  labs(title = "Posterior density of mu") +
  theme_bw()

posterior |>
  ggplot(aes(x = mu)) +
  stat_ecdf(col = bayes_col["posterior"]) +
  labs(title = "Posterior cdf of mu",
       y = "Cumulative probability") +
  theme_bw()

```


Quantiles of the posterior distribution of $\mu$ are endpoints of credible intervals.

```{r}
posterior |>
  summarise(enframe(quantile(mu, c(0.01, 0.10, 0.25, 0.75, 0.90, 0.99)), "quantile", "mu")) |>
  kable()

```


We can approximate posterior probabilities as usual.

```{r}
sum(posterior$mu > 60) / length(posterior$mu)

```

:::





# Posterior predictive checking (~5 minutes) {#sec-pp-check}

In the previous section, we used software to find the posterior distribution of parameters.
The posterior distribution of $\mu$ is approximately Normal with mean `r round(mean(posterior$mu), 0)` minutes and standard deviation `r round(sd(posterior$mu), 1)` minutes, and the posterior distribution of $\sigma$ is approximately Normal with mean `r round(mean(posterior$sigma), 0)` minutes and standard deviation `r round(sd(posterior$sigma), 1)` minutes.
(*And $\mu$ and $\sigma$ have an approximate joint Normal distribution with correlation `r round(cor(posterior$mu, posterior$sigma), 2)`.*)

Now we will use simulation to approximate the posterior predictive distribution, and use this distribution to assess the assumptions of our model.

::: {.panel-tabset}
## Questions


1.  Recall that in @sec-prior-predictive we simulated the prior predictive distribution of arrival times, prior to observing data.
Describe how you could simulate the **posterior predictive distribution** of arrival times, after fitting the model to the data.
*We will discuss prediction later; for now we'll focus on predictive checking.*


1.  The sample data is summarized below.
(Remember, we should have done this before fitting the model!)
Do you think our model is reasonable?
Why?


    ```{r}
    #| ref.label: !expr c("data-plot")
    #| echo: false
    ```


1.  The posterior predictive distribution represents the distribution of arrival times according to the model fit based on the data.
We can perform a visual **posterior predictive check** (`pp_check`) to see if the model is reasonable by comparing hypothetical samples (light blue) simulated from the posterior predictive distribution to the observed sample (dark blue).
What do the plots below say about the appropriateness of our model?


```{r}
#| ref.label: !expr c("pp-check-hist", "pp-check")
#| echo: false
#| layout-ncol: 2
#| fig-cap:
#|   - "Histograms of sample data (dark blue) and of 10 hypothetical samples simulated from the posterior predictive distribution (light blue)"
#|   - "Density plots of sample data (dark blue) and of 100 hypothetical samples simulated from the posterior predictive distribution (light blue)"

```



## Solution and Discussion


1.  The simulation is similar to the prior predictive distribution; but now we simulate $(\mu, \sigma)$ pairs from their posterior distribution.
First, simulate a $(\mu, \sigma)$ pair from the posterior distribution; software has already done this for us---we have 10000 simulated pairs.
For each $\mu, \sigma$ pair, simulate an arrival time $y$ from a $N(\mu, \sigma)$ distribution.
Summarize the simulated $y$ values to approximate the posterior predictive distribution.


1.  The sample data is clearly skewed to the right, so a conditional Normal model is probably not appropriate.


1.  If there is a good fit, then replicated data generated under the model should look similar to the observed data.
But here all of the simulated samples are relatively symmetric, while the observed sample data is skewed.
The observed data seems inconsistent with what our model predicts, which indicates the fit is poor.
(“Based on the data we observed, we conclude that it would be unlikely to observe the data we observed???”)


## Code


Data is summarized as usual.

```{r}
#| label: data-plot

data |>
  ggplot(aes(x = minutes)) +
  geom_histogram(aes(y = after_stat(density)), col = "black", fill = "white") +
  geom_density(linewidth = 2) +
  theme_bw()

```

```{r}
#| label: data-plot2
#| include: false
#| eval: false
#| echo: false

data |>
  ggplot(aes(x = minutes)) +
  geom_histogram(aes(y = after_stat(count / sum(count))),
                     col = "black", fill = "#E69F00", bins = 20) +
  scale_x_continuous(breaks = seq(-50, 300, 50)) +
  labs(x = "Arrival time (minutes after start)",
       y = "Percent of arrivals") +
  scale_y_continuous(labels = scales::percent) +
  theme_bw() +
  theme(
    axis.title.x = element_text(size = 14, face = "bold"),
    axis.title.y = element_text(size = 14, face = "bold"),
        axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12)
  )

```


We can perform a visual posterior predictive check (`pp_check`) to see if the model is reasonable by comparing hypothetical samples simulated from the posterior predictive distribution to the observed sample.


```{r}
#| label: pp-check-hist
pp_check(fit, type = "hist")
```


```{r}
#| label: pp-check

pp_check(fit, ndraws = 100)

```


Built-in functions like `posterior_predict` can be used to compute predicted values.
The following plot displays the approximate posterior predictive distribution. 

```{r}
data.frame(y_predict = posterior_predict(fit, newdata = 1)) |>
  ggplot(aes(x = y_predict)) +
  geom_density(col = bayes_col["post_predict"]) + 
  theme_bw()
```

We could also use the $(\mu, \sigma)$ values simulated from the posterior distribution---extracted earlier using `spread_draws`---to simulate predicted $y$ values from a conditional $N(\mu, \sigma)$ distribution.

```{r}
posterior_and_predictive <- posterior |>
  mutate(y_predict = rnorm(10000, mu, sigma))

posterior_and_predictive |>
  head() |>
  kable()
```


```{r}
posterior_and_predictive |>
ggplot(aes(x = y_predict)) +
  geom_density(col = bayes_col["post_predict"]) +
  labs(title = "Posterior predictive distribution",
       x = "Predicted arrival time") +
  theme_bw()
```



::: {.callout-note}
To perform posterior predictive checks, we simulate several samples of the same size as the observed data.
To approximate the posterior predictive distribution---which is like a population distribution---we simulate many (e.g., 10000) predicted values of the response variable, one predicted value for each simulated parameter vector.
:::

:::

# Posterior prediction (~15 minutes) {#sec-prediction}


The sample data suggests a model that allows for more skewness would be more appropriate.
This primarily results in a change of the likelihood function.
There are several likelihood functions we can use, but we'll try a [Skew Normal](https://en.wikipedia.org/wiki/Skew_normal_distribution) family, which introduces an additional skewness parameter $\alpha$.
Since $\alpha$ is a parameter it also needs a prior distribution.
Rather than redoing our prior predictive tuning, we'll rely on the fact that software packages like `brms` can utilize default priors.
The code which fits the Skew Normal model to the data and some output is in the Code section; we basically just change `family = gaussian` to `family = skew_normal` in `brm`.

```{r}
#| label: brm-fit2
#| cache: true
#| echo: false

fit2 <- brm(data = data,
           family = skew_normal,
           minutes ~ 1,
           prior = c(prior(normal(40, 15), class = Intercept),
                     prior(gamma(30, 30 ^ 2 / 10 ^ 2), class = sigma)), # gamma(mean, shape)
           iter = 11000,
           warmup = 1000,
           chains = 1,
           refresh = 0)

```

::: {.callout-note}
Especially when first encountering Bayesian analyses, outsized attention is often paid to the prior distribution.
It's important to remember that the prior is only one part of the Bayesian model.
The likelihood is the other part, which also involves many assumptions.
And there is the data that is used to fit the model, and all the considerations that go into how the data were collected.
We like to introduce early an example like this one where the assumptions in the likelihood are invalid to emphasize that choice of prior is just one of many modeling assumptions that should be evaluated and checked.
:::



::: {.panel-tabset}
## Questions


1.  We can perform a visual **posterior predictive check** to see if the Skew Normal model is reasonable by comparing hypothetical samples (light blue) simulated from the posterior predictive distribution to the observed sample (dark blue).
What do the plots below say about the appropriateness of the model?
Does the Skew Normal seem more reasonable than the Normal model from @sec-pp-check?


```{r, indent = "    "}
#| ref.label: !expr c("pp-check-hist2", "pp-check2")
#| echo: false
#| layout-ncol: 2
#| fig-cap:
#|   - "Histograms of sample data (dark blue) and of 10 hypothetical samples simulated from the posterior predictive distribution (light blue)"
#|   - "Density plots of sample data (dark blue) and of 100 hypothetical samples simulated from the posterior predictive distribution (light blue)"

```




1.  The plot below displays the **posterior predictive distribution**.
Find and interpret in context a 95% **prediction interval**.


```{r, indent = "    "}
#| label: fig-posterior-predictive
#| fig-cap: "Posterior predictive distribution"
#| echo: false

post_predict <- data.frame(y_predict = posterior_predict(fit2, newdata = 1))

p1 = post_predict |>
  ggplot(aes(x = y_predict)) +
  geom_density(col = bayes_col[4]) +
  labs(x = "Arrival time (minutes)",
       title = "Simulated posterior predictive distribution of arrival times: Density") +
  scale_x_continuous(breaks = seq(-60, 180, by = 30),
                     limits = c(-60, 180)) +
  theme_bw()

p2 = post_predict |>
  ggplot(aes(x = y_predict)) +
  stat_ecdf(col = bayes_col[4]) +
  labs(x = "Arrival time (minutes)",
       y = "Cumulative probability",
       title = "Simulated posterior predictive distribution of arrival times: CDF") +
  scale_x_continuous(breaks = seq(-60, 180, by = 30),
                     limits = c(-60, 180)) +
  theme_bw()

grid.arrange(p1, p2)


```



1.  What percent of party-goers are predicted to arrive before the start time of the party?


## Solution and discussion


A posterior predictive check reveals that the Skew Normal model is much more appropriate than the Normal model.
(The Skew Normal model might not be the best, but we should also be wary of overfitting the data.)


```{r}
#| echo: false

post_pred_int = predictive_interval(fit2, prob = 0.95, newdata = 1)
```


Based on this model we predict that 95% of arrival times are between `r round(post_pred_int[1], 0)` and `r round(post_pred_int[2], 0)` minutes.
Roughly, 95% of party-goers arrive between `r -round(post_pred_int[1], 0)` minutes before the party start time and `r round(post_pred_int[2], 0)` minutes after the party start time.

```{r}
#| echo: false

post_pred_prob = sum(post_predict$y_predict < 0) / length(post_predict$y_predict)


```


About `r 100 * round(post_pred_prob, 3)`% of party-goers are predicted to arrive before the party start time.

## Code


The `brm` code is similar to above, with `gaussian` replaced by `skew_normal`.
Note that we have not specified a prior for $\alpha$, so a default prior will be used.

```{r}
#| ref.label: !expr c("brm-fit2")

```



Note that the posterior distribution now quantifies the uncertainty in three parameters.


```{r}

plot(fit2)

```

```{r}

pairs(fit2,
      off_diag_args = list(alpha = 0.1))
```

```{r}

summary(fit2)

```


Here is a graphical posterior predictive check.


```{r}
#| label: pp-check-hist2
pp_check(fit2, type = "hist")
```


```{r}
#| label: pp-check2

pp_check(fit2, ndraws = 100)

```


Here is a plot of the posterior predictive distribution.

```{r}
#| ref.label: !expr c("fig-posterior-predictive")

```




We could use quantiles of the predicted values or built in functions (`predictive_interval`) to find a predictive interval.

```{r}
quantile(post_predict$y_predict, c(0.025, 0.975))
```


```{r}
#| eval: false
predictive_interval(fit2, prob = 0.95, newdata = 1)
```

```{r}
#| echo: false
post_pred_int
```


We can use simulated values to approximate probabilities as usual.

```{r}
sum(post_predict$y_predict < 0) / length(post_predict$y_predict)
```



::: {.callout-note}
It is common to compute *central* credible intervals and prediction intervals, but it's certainly not necessary.
Posterior and posterior predictive distributions can be summarized in whatever way is most meaningful in context.
:::

:::

# Wrap up (~5 minutes) {#sec-wrap-up}

::: {.panel-tabset}
## Summary


- Most interesting problems involve multiple parameters, so prior/posterior distributions are multivariate distributions.
- The posterior distribution is almost always approximated using MCMC simulation algorithms and software.
- The main inputs to an MCMC algorithm are: data, likelihood, and prior
- MCMC algorithms implement Bayesian updating---posterior is proportional to the product of prior and likelihood---in a computationally efficient way.
- Prediction is natural in Bayesian analyses.
- Prior predictive distributions can help tune prior distributions of parameters.
- Posterior predictive distributions can help diagnose the model fit, and can be used for making predictions (e.g., prediction intervals).


## Comments


- Students don't necessarily need to know how MCMC algorithms work.
Module 1 illustrated the basic process of posterior simulation and the Bayesian paradigm.
Just tell students that MCMC algorithms implement the process more efficiently.
- Students should focus on specifying the input (data, likelihood, prior) and interpreting the output (posterior).
- Students don't necessarily need to code; you can just give them summaries of the posterior distribution.
- Can teach Bayesian statistics using a simulation-based approach.
Even if students aren't coding, we still like to ask lots of "how would you simulate this?" questions to assess understanding of the process.
- We also conduct lots of tactile simulations in class, using spinners.
- Students can easily get confused by all the distributions.
Be careful to distinguish between prior/posterior distributions of parameters and prior/posterior predictive distributions of values of the measured variables.
- Be careful to distinguish between all the different standard deviations.
Ask lots of questions like "this SD measures variability/uncertainty of what?".
- Bayesian analyses are inherently multivariable (because parameters are variables), giving students lots of experience with multivariable thinking (a GAISE guideline).
- Choice of prior is just one assumption.
Don't spend too much time on choosing the prior.
It's the posterior that matters!
- Remember, Bayesian statistics is statistics.
The same best practices that you use in other classes still apply.
In particular, focus on conceptual understanding rather than calculus, computation, or coding. 


## Further topics to investigate with students


- Many of the same questions from Module 1, like how sensitive is the posterior to choice of prior?
- Choice of likelihood to reflect skewness and the Skew Normal model.
- Revise the inference about $\mu$ based on the Skew Normal model.
- Cover credible intervals versus prediction intervals in more detail.
- What might explain the variability in arrival times (e.g., size of party)?
Could bridge to regression or hierarchical models.
- Compare Bayesian with frequentist.
- Be sure to also include plenty of "usual" statistics questions.
For example, the party time data provides a good opportunity to discuss mean versus median.


## Resources

- `brms` is an interface for the MCMC software [Stan](https://mc-stan.org/).
- Here are a few references for using `brms`
    - [`brms`](https://paul-buerkner.github.io/brms/) site
    - [*Bayesian Modeling using Stan*](https://bayesball.github.io/BRMS/) by Jim Albert (companion to Albert and Hu textbook)
    - [*Doing Bayesian Data Analysis in brms and the tidyverse*](https://bookdown.org/content/3686/) by A Solomon Kurz (companion to Kruschke's *Doing Bayesian Data Analysis*)
    - [Parameterization of Response Distributions in `brms`](https://cran.r-project.org/web/packages/brms/vignettes/brms_families.html)
    - [`brms` reference manual](https://cran.r-project.org/web/packages/brms/brms.pdf)
- [JASP](https://jasp-stats.org/) is point-and-click software for performing some Bayesian (and frequentist) analyses
- [ShinyStan](https://mc-stan.org/users/interfaces/shinystan) is a nice interactive tool for exploring output of a Bayesian analysis.

:::



